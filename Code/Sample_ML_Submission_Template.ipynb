{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["w6K7xa23Elo4","mDgbUHAGgjLW","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - Zomato Restaurant Analysis Project\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Unsupervised\n","##### **Contribution**    - Individual\n","##### **Team Member 1 -** Vivek Lamba"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["The data science problem focuses on approaching a business challenge analytically while applying critical thinking. Among various machine learning approaches, Unsupervised Learning is widely used, particularly for clustering, association mining, and dimensionality reduction.\n","\n","In this project, the primary objective is to perform restaurant clustering and then conduct sentiment analysis to understand customer opinions from their reviews.\n","\n","The goal of this project is to analyze Zomato restaurant data across various Indian cities, group restaurants into meaningful clusters, and further analyze customer reviews to determine whether the sentiment toward each restaurant is positive or negative.\n","\n","Before performing any analysis, it is essential to understand the dataset thoroughly. Therefore, initial exploratory steps such as .head() and .info() were used to observe structure, column types, and basic characteristics of the data.\n","\n","Once familiarized with the dataset, data wrangling was performed to clean and prepare the data. This included converting the cost field into numerical format, handling invalid rating values, and correcting inconsistencies.\n","\n","After preprocessing, the next step was Exploratory Data Analysis (EDA) to uncover deeper insights. Various visualizations and comparisons were generated to understand restaurant characteristics and customer behavior patterns.\n","\n","Certain assumptions and questions regarding the data were tested through Hypothesis Testing, where p-values and significance levels were used to accept or reject hypotheses.\n","\n","Following this, feature engineering was carried out to prepare data for modeling. This involved handling missing values, addressing outliers, scaling features, extracting relevant variables, and transforming data where needed.\n","\n","Clustering was first performed on the restaurant dataset. Before clustering, Principal Component Analysis (PCA) was applied for dimensionality reduction. Three clustering algorithms were implemented:\n","K-Means Clustering\n","Agglomerative Hierarchical Clustering\n","\n","Based on results and Silhouette scores, it was observed that the dataset could be effectively grouped into six clusters.\n","\n","Next, sentiment analysis was carried out on the review dataset. Comprehensive text preprocessing was applied, including punctuation removal, stopword removal, lemmatization, emoji handling, lowercase conversion, and tokenization using TF-IDF vectorization.\n","Multiple models were evaluated:\n","Logistic Regression\n","Decision Tree\n","Random Forest\n","XGBoost\n","K-Nearest Neighbors\n","Logistic Regression delivered the best performance based on AUC-ROC score. Hyperparameter tuning further confirmed logistic regression as the most suitable model for final sentiment prediction."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["**The rapid growth of online food delivery platforms such as Zomato has resulted in massive volumes of restaurant data and customer reviews. This data contains valuable insights regarding customer satisfaction, restaurant performance, pricing perception, and service quality. However, in its raw form, this information is unstructured and difficult to interpret for meaningful business decisions.\n","\n","The objective of this project is to analyze Zomato restaurant data to understand how restaurant pricing influences customer ratings, identify meaningful restaurant clusters based on review patterns, and perform sentiment analysis on customer reviews to evaluate their opinions. Specifically, the project aims to:\n","Analyze the relationship between restaurant cost and customer ratings and determine whether premium restaurants receive significantly higher ratings than budget restaurants.\n","\n","Cluster restaurants based on review text similarity to understand behavioral grouping and similarity in customer perception.\n","\n","Perform sentiment analysis on user reviews to classify customer feedback into positive, negative, or neutral sentiment categories.\n","\n","Provide business insights that help restaurants improve service quality, pricing strategies, and customer experience.\n","\n","Through statistical analysis, machine learning techniques, clustering methods, and natural language processing, this project seeks to transform raw Zomato data into actionable intelligence that benefits customers, restaurant owners, and platform stakeholders.**"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import datetime as dt\n","from datetime import datetime\n","\n","from wordcloud import WordCloud\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading Zomato Restaurant names and Metadata Dataset\n","hotels=pd.read_csv('/content/drive/MyDrive/Zomato Project/Zomato Restaurant names and Metadata.csv')\n","\n","#Loading Zomato Restaurant reviews Dataset\n","reviews=pd.read_csv('/content/drive/MyDrive/Zomato Project/Zomato Restaurant reviews.csv')\n","\n"],"metadata":{"id":"BGKWnZnk3oKK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","hotels.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews.head()"],"metadata":{"id":"aPw065qk4E10"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","hotels.shape"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews.shape"],"metadata":{"id":"-LGstm6u4Nn_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hotels contains 105 records and 6 features while reviews dataset contains 10000 records and 7 features."],"metadata":{"id":"9EJ5lOMa4STh"}},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","hotels.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cost must be int type but it contains comma(,) , hence its datatype is object here. Also Timings represent the time from when the restaurant opens till end time when restaurants shut down, it is given in the form of text, hence object datatype."],"metadata":{"id":"LBt4ko_K4bfh"}},{"cell_type":"code","source":["reviews.info()"],"metadata":{"id":"v7uhA33p4V9q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here all the columns in both the dataset is 'object' type except 'pictures'"],"metadata":{"id":"89eZlP6i4hSG"}},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","hotels.duplicated().sum()"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#review Dataset Duplicate Value Count\n","reviews.duplicated().sum()"],"metadata":{"id":"7oyHEkee4nYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check what are dplicated values present in the dataset\n","reviews[reviews.duplicated()]"],"metadata":{"id":"rIIPKx4P4upB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since all the duplicated rows are null values. Hence we can drop them."],"metadata":{"id":"hWhVeij74zVn"}},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","hotels.isnull().sum()"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","sns.heatmap(hotels.isnull(), cbar=False)"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are some missing values in the column collections a, i.e., 54 and one in Timings column."],"metadata":{"id":"m0tUVUZ65SSj"}},{"cell_type":"code","source":["# Missing Values/Null Values Count in review\n","reviews.isnull().sum()"],"metadata":{"id":"il1HQp4L5Vpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","sns.heatmap(reviews.isnull(), cbar=False)"],"metadata":{"id":"zhoaepIZ5V1V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In reviews dataset, most of the columns have missing values."],"metadata":{"id":"1oJySSdW5WMy"}},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["There are two datasets given:\n","\n","\n","\n","1.   Restaurant Names and metadata:\n","\n","*   There are 105 records and 6 features in metadata.\n","*   There are missing or null values in Colllections and timings.\n","*   There are no duplicated values.\n","*   Cost must be int type but it contains comma(,) , hence its datatype  is  object here.\n","*   Timings represent the time from when the restaurant opens till end time when restaurants shut down.\n","\n","\n","2.   Reviews dataset:\n","\n","*   There are 10000 records given with 7 features.\n","*   Except Name of Restaurants and Number of picture posted, There are null values.\n","*   There are some of the duplicated values for restaurnts which can be dropped.\n","*   Rating must be integer but it contais value 'like', hence it is object datatype.\n","\n","\n","\n"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","hotels.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Columns\n","reviews.columns"],"metadata":{"id":"4ZXhwyZ_7X32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","hotels.describe(include='all').T"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","\n","*   Here none of the columns are seemed to be categorical.\n","*   Majority of the columns have unique values.\n","\n"],"metadata":{"id":"PqclJsfy71qi"}},{"cell_type":"code","source":["# Dataset Describe\n","reviews.describe(include='all').T"],"metadata":{"id":"uvwSTAMq7lcX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","*   From description of dataset, we can infer that there are 100 unique Restaurants for which customers have given their review.\n","*   Some of the reviewers or customer have given review to more than 1 restaurant.\n","\n","\n","\n"],"metadata":{"id":"BOXVUoE272zg"}},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["**Zomato Restaurant**\n","* Name : Name of Restaurants\n","\n","* Links : URL Links of Restaurants\n","\n","* Cost : Per person estimated Cost of dining\n","\n","* Collection : Tagging of Restaurants w.r.t. Zomato categories\n","\n","* Cuisines : Cuisines served by Restaurants\n","\n","* Timings : Restaurant Timings\n","\n","**Zomato Restaurant Reviews**\n","* Restaurant : Name of the Restaurant\n","\n","* Reviewer : Name of the Reviewer\n","\n","* Review : Review Text\n","\n","* Rating : Rating Provided by Reviewer\n","\n","* MetaData : Reviewer Metadata - No. of Reviews and followers\n","\n","* Time: Date and Time of Review\n","\n","* Pictures : No. of pictures posted with review"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","hotels.nunique()"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","reviews.nunique()"],"metadata":{"id":"oS7ZUCHT9efm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews['Rating'].unique()"],"metadata":{"id":"AgN8wRjU-JDi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The ratings are from 0-5 but the customers have given in .5. Therefore they are going to 10 and their are like and null values also. We can replace the null value with the median.And we need to replace the like as well, so we can replace it with 3.5."],"metadata":{"id":"WgTo5T9L-TuS"}},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"markdown","source":["Hotel Dataset"],"metadata":{"id":"MBSGead4Ay_y"}},{"cell_type":"code","source":["# Renaming the hotel dataset column name\n","hotels.rename(columns={'Name':'Restaurant'},inplace=True)"],"metadata":{"id":"xtQ-_5bjAzfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking values for cost\n","hotels['Cost'].unique()"],"metadata":{"id":"XtSc4M_tAztn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing ',' from Cost\n","hotels['Cost']=hotels['Cost'].str.replace(\",\",\"\").astype(\"int64\")"],"metadata":{"id":"wuATfvBPAzxz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function for number of cuisine in a hotel\n","def no_of_cuisine(cuisine):\n","  Cuisine_list=list(str(cuisine).split(','))\n","  return len(Cuisine_list)\n","\n","# Create a new column with no of cuisine in hotel dataframe\n","hotels['No_of_cuisine']=hotels['Cuisines'].apply(no_of_cuisine)"],"metadata":{"id":"IncQ316RAz17"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Review Dataset"],"metadata":{"id":"KxrXfYVHAqqL"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# Dropping the duplicate values in reviews df\n","reviews.drop_duplicates(keep=False,inplace=True)"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace Rating 'Like' with rating 4\n","reviews['Rating']=reviews['Rating'].str.replace(\"Like\",'3.5').astype('float')"],"metadata":{"id":"Lw0yh3Rh-_2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#splitting the metadata into Reviews and Followers\n","reviews[['No_of_reviews','Followers']] = reviews['Metadata'].str.split(',', expand=True)\n","reviews['No_of_reviews'] = pd.to_numeric(reviews['No_of_reviews'].str.split(' ').str[0])\n","reviews['Followers'] = pd.to_numeric(reviews['Followers'].str.split(' ').str[1])\n","reviews.head()"],"metadata":{"id":"dhinGFcY_AaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filling the null values of Followes with 0\n","reviews['Followers'].fillna(0,inplace=True)"],"metadata":{"id":"GM6A5fGSATJT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting Time to date time and extracting Hour and year\n","reviews['Time']=pd.to_datetime(reviews['Time'])\n","reviews['Year']=pd.DatetimeIndex(reviews['Time']).year\n","reviews['Hour'] = pd.DatetimeIndex(reviews['Time']).hour"],"metadata":{"id":"bo5RYWbbATNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews.info()"],"metadata":{"id":"TLaY-rU4ATRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new column for average rating in hotel dataset\n","Average_rating = reviews.groupby(by='Restaurant',as_index='False')['Rating'].mean().reset_index()\n","Average_rating.rename(columns={'Rating':'Average_rating'},inplace = True)\n","Average_rating.head()\n"],"metadata":{"id":"NmaUAwduAz5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's merge the average rating with hotel dataset\n","hotels = hotels.merge(Average_rating,on = 'Restaurant')"],"metadata":{"id":"Bsr6kw44Etv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's merge the two dataset\n","df = hotels.merge(reviews, left_on = 'Restaurant',right_on='Restaurant')\n","df.shape"],"metadata":{"id":"tF9lEmDGF1Ok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["For the Hotel dataset:\n","\n","*   Rename the Column 'Name' to 'Restaurant'.\n","*   Removed comma(,) from Cost and changed its datatype to integer.\n","*   Formed the function for the number of cuisines.\n","*   Merged the average rating in hotel dataset.\n","\n","For the Review dataset:\n","\n","*   Dropped the duplicate rows.\n","*   Changed the Rating - Like to numeric value and changed it datatype.\n","*   Extracted No_of_review and followers from Metadata column and filled the null values of followes with 0.\n","*   Changed the time datatype to datetime and extracted Year and Hour from it."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","\n","# swarmplot to see the variation in price\n","sns.boxplot( y=\"Cost\", data=hotels)"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["\n","To find the cost of restaurants.\n"],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["It is clearly visible that average cost per person in restaurants varies from below 500 to more than 2500. But there are few restaurants whose price is more than 2000."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["The analysis clearly shows that restaurant pricing varies widely — most restaurants fall under budget to mid-range pricing (below 500 to 1500), while only a small segment lies above 2000 per person.\n","\n","Since only a few restaurants operate successfully above 2000, blindly increasing prices without improving value may reduce customers, attract negative reviews, and harm business performance."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","#Find out the costliest and cheapest restaurants\n","costly_res=hotels[['Restaurant','Cost']].groupby('Restaurant',as_index=False).mean().sort_values(by='Cost',ascending=False).head(5).reset_index(drop=True)\n","cheapest_res = hotels[['Restaurant','Cost']].groupby('Restaurant',as_index=False).mean().sort_values(by='Cost',ascending=True).head(5).reset_index(drop=True)\n","print(costly_res.head())\n","print('\\n',cheapest_res.head())"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualisation of most expensive and cheapest restaurant\n","fig,axes=plt.subplots(nrows=1,ncols=2,constrained_layout=True,figsize=(14,7))\n","\n","#costliest restaurant\n","a=sns.barplot(x = 'Restaurant',y = 'Cost',data = costly_res,ax = axes[0],palette = 'plasma')\n","a.set_xticklabels(labels=costly_res['Restaurant'].to_list(),rotation=90)\n","\n","#cheapest restaurant\n","b=sns.barplot(x = 'Restaurant',y = 'Cost',data = cheapest_res,ax = axes[1],palette = 'plasma')\n","b.set_xticklabels(labels=cheapest_res['Restaurant'].to_list(),rotation=90)\n","plt.show()"],"metadata":{"id":"MwG9t1n6J_1l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["To visualize which are the expensive restaurants and which are the cheap restaurants avaliable on Zomato."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Expensive Restaurants : Here \"Collage - Hyatt Hyderabad Gachibowli\" is the most expensive restaurant whose price is 2800 which is followed by \"Feast - Sheraton Hyderabad Hotel\" whose price is rupees 2500.\n","\n","Cheap Restaurants : Here \"Mohammedia Shawarma\" and \"Amul\" is the cheapest restaurant where we can get the dish with the minimum price of rupees 150 , which is followed by \"Hunger Maggi Point\", \"Asian Meal Box\", \"Momos Delight etc whose price is rupees 200 ."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["The gained insights can create a positive business impact because they help clearly identify premium restaurants for high-end customers and budget restaurants for price-sensitive users.\n","\n","This allows better pricing strategies, targeted marketing, and improved customer segmentation.\n","\n","However, highlighting very expensive restaurants may discourage budget users, and extremely cheap restaurants may struggle with service quality and profit margins if demand rises.\n","\n","Therefore, while insights are beneficial, businesses must balance pricing, quality, and customer expectations to avoid negative growth."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["from numpy.random.mtrand import normal\n","# Chart - 3 visualization code\n","plt.figure(figsize = (18,8));\n","for i,col in enumerate(['Cost','Rating','Year']) :\n","    # plt.figure(figsize = (8,5));\n","    plt.subplot(2,2,i+1);\n","    sns.histplot(df[col], kde=True, color='#055E85');\n","    feature = df[col]\n","    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n","    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n","    plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\n","    plt.title(f'{col.title()}');\n","    plt.tight_layout();"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["Histplot is helpful in understanding the distribution of the feature."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["* All three are show skewness.\n","* Maximum restaurant show price range for 500.\n","* In 2018 number of reviews are more."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Price always place important role in any business alongwith rating which show how much engagement are made for the product.\n","\n","But in this chart it is unable to figure any impact on business when plotted all alone."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","#CREATING WORDCLOUD FOR EXPENSIVE RESTAURANT\n","from wordcloud import WordCloud\n","plt.figure(figsize=(20,10))\n","text = \" \".join(name for name in hotels.sort_values('Cost',ascending=False).Restaurant[:30])\n","\n","# Creating word_cloud with text as argument in .generate() method\n","word_cloud = WordCloud(width = 2000, height = 2000,collocations = False,\n","                       colormap='rainbow',background_color = 'black').generate(text)\n","\n","# Display the generated Word Cloud\n","plt.imshow(word_cloud, interpolation='bilinear');\n","plt.axis(\"off\")\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CREATING WORDCLOUD FOR CHEAPEST RESTAURANT\n","plt.figure(figsize=(15,8))\n","text = \" \".join(name for name in hotels.sort_values('Cost',ascending=True).Restaurant[:30])\n","\n","# Creating word_cloud with text as argument in .generate() method\n","wordcloud = WordCloud(background_color=\"white\").generate(text)\n","# Display the generated Word Cloud\n","plt.imshow(wordcloud, interpolation='bilinear');\n","plt.axis(\"off\")"],"metadata":{"id":"dGe01jBDQaKq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["I used Wordcloud because it show all text and highlight the most frequent words."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["From the above chart, HYDERABAD, HOTEL, BAR etc seems frequently repeating for expensive restaurant, while for cheap restaurants SHAWARMA, DHABA, RESTAURANTS seems frequently repeating. So it can be infer that Hotel and Bars of Hyderabad are expensive while Dhabas and Restaurants are cheaper."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["The insights can positively impact business by helping restaurants position themselves correctly—premium Hotels and Bars can justify higher pricing through enhanced service, while budget Dhabas and Restaurants can focus on affordability and high-volume strategies. These insights also support better market segmentation, targeted marketing, and improved customer recommendations.\n","\n","However, negative growth is possible if premium outlets overprice without delivering quality, or if budget restaurants remain stuck in a low-price perception without improving service standards, which may lead to customer dissatisfaction and lost business."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","#average rating and total number of review given to the restaurants\n","\n","avg_hotel_rating = reviews.groupby('Restaurant').agg({'Rating':'mean',\n","        'Reviewer': 'count'}).reset_index().rename(columns = {'Reviewer': 'Total_Review'})\n","avg_hotel_rating"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axes=plt.subplots(nrows=1,ncols=2,constrained_layout=True,figsize=(14,7))\n","\n","# Let's see te histogram of average rating\n","a=sns.histplot(data=avg_hotel_rating['Rating'],bins=20,kde=True,ax=axes[0])\n","\n","# plot the pie chart of number of reveivers for restaurants\n","b=avg_hotel_rating['Total_Review'].value_counts().plot(kind='pie', shadow=False, autopct='%1.02f%%',\n","                                                       explode = (0.001, 0.5, 0.5),pctdistance=1.1,labeldistance=1.20,\n","                                                       colors=['green','red','purple'],ax=axes[1])\n","plt.show()"],"metadata":{"id":"VOVpcssASswn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["To see the distribution of average rating , I used histplot and to see review distribution, I used pie chart."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["average Ratings are normally distributed for the restaurants.\n","\n","100 reviews are given to all the restaurants except 2 restaurants whose reviews are 85 and 77 respectively."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["The insights can positively impact business decisions since ratings are normally distributed and backed by sufficient reviews, making them reliable for strategic planning. However, the two restaurants with significantly fewer reviews may lead to misleading conclusions and potential negative business impact if treated equally without considering review volume."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["#Most popular cuisines\n","cuisine_list=[]\n","cuisines=hotels.Cuisines.str.split(',')\n","\n","#Get all the cuuisines in a list\n","for i in cuisines:\n","  for j in i:\n","    cuisine_list.append(j)\n","\n","# converting it to dataframe\n","cuisine_series=pd.Series(cuisine_list)\n","cuisine_df=pd.DataFrame(cuisine_series,columns=['Cuisines'])\n","cuisine_df[cuisine_df['Cuisines']==' North Indian']='North Indian'\n","#cuisine_df\n","cuisine_=pd.DataFrame(cuisine_df.groupby(by='Cuisines',as_index=False).value_counts())\n","cuisine_"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.barplot(x='count', y='Cuisines', data=cuisine_.sort_values(ascending=False, by='count')[:10],palette='crest')\n","plt.title('10 Most Famous Cuisine')\n","plt.show()"],"metadata":{"id":"EjRN2JQkWEi-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["Since categorical features are best visualized through bar chart."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["From the result of the graph it is clearly visible that North Indian is the most served cuisine in restaurants which is followed by Chinese then Continental."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Helps businesses understand that North Indian, Chinese, and Continental cuisines have the highest demand, enabling better menu planning, investment decisions, and targeted marketing strategies.\n","\n","However, the dominance of North Indian cuisine indicates high competition, which makes it difficult for new or smaller restaurants to stand out and grow."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","#CREATING WORDCLOUD FOR CUISINES\n","\n","plt.figure(figsize=(12,10))\n","df_word_cloud = cuisine_df['Cuisines']\n","text = \" \".join(word for word in df_word_cloud)\n","\n","# Generate a word cloud image\n","wordcloud = WordCloud(background_color=\"white\").generate(text)\n","# Display the generated image:\n","# the matplotlib way:\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["It show all text and highlight the most frequent words."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["From the above chart, North Indian is the most frequently used which is followed by chinese and continental."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["the word cloud highlights dominant cuisines like North Indian, Chinese, Italian, Fast Food, and Continental, helping businesses understand popular customer preferences and plan menus, marketing, and offerings accordingly.\n","\n","However, over-reliance on these popular cuisines may lead to saturation and intense competition, while under-represented cuisines risk being ignored, potentially missing market opportunities."],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","# Creating word cloud for reviews\n","plt.figure(figsize=(15,10))\n","text = \" \".join(name for name in reviews.sort_values('Review',ascending=False).Review[:30])\n","\n","# Creating word_cloud with text as argument in .generate() method\n","word_cloud = WordCloud(width = 1400, height = 1400,collocations = False, background_color = 'black').generate(text)\n","\n","# Display the generated Word Cloud\n","plt.imshow(word_cloud, interpolation='bilinear')\n","\n","plt.axis(\"off\")"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["To see what word is frequently used by the reviewers."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["Most of the time customers liked the food because good is repeating most in reviews. Also food,place,restaurant are next most repeating word."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["The word cloud shows frequent positive terms like good, tasty, amazing, nice, perfect, service, indicating strong customer satisfaction. Businesses can use this information to keep improving their food quality and service, and they can also highlight these positive experiences in their marketing to attract more customers.\n","\n","Some small negative words like less, wrong, leaking, spicy, and delayed show that a few customers are unhappy with service or food quality at times. If these issues are not fixed, customers may lose trust, give bad reviews, and may not want to come back."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["plt.figure(figsize = (15,8))\n","sns.countplot(\n","    x='Collections',\n","    data=hotels,\n","    order=hotels.Collections.value_counts().head(10).index,\n","    palette=\"viridis\"\n",")\n","plt.title('Count of collections')\n","plt.xticks(rotation = 90)\n","plt.show()"],"metadata":{"id":"lUV5KYyVakkw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["To check the count of each collections."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["Here Food Hygiene Rated Restaurants in Hyderabad has the maximum count of 4 which is followed by Hyderabad Hottest,New on Gold etc."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["This chart helps businesses understand which types of restaurant collections, like Food Hygiene Rated, Veggie Friendly, and Trending This Week, are more popular. Restaurants can use this information to improve their positioning, join popular categories, and create better marketing plans to attract more customers.\n","\n","Some collections have very few restaurants, which shows low participation or awareness. If restaurants ignore these less-represented categories, they might miss good opportunities and lose potential customers who are interested in those specific themes."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["#numerical columns for hotel dataset\n","num_cols_hotel = ['Cost', 'No_of_cuisine']"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Distribution plot for hotel dataset\n","n=1\n","plt.figure(figsize=(10,7))\n","for col in num_cols_hotel:\n","   plt.subplot(1,2,n)\n","   n+=1\n","   sns.distplot(hotels[col])\n","   plt.title(col)\n","   plt.tight_layout()"],"metadata":{"id":"0j3nxOX5Bq6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["To see the distribution of numerical columns.\n","\n","\n"],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["Most restaurants fall in the mid-price range, while only a few are very expensive. Also, most restaurants serve 2–4 cuisines, indicating they maintain variety without overcomplicating menus."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["These insights help restaurants set competitive pricing and design balanced menus, while avoiding very high costs or too many cuisines that may negatively impact customer satisfaction and growth."],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["#numerical columns for review dataset\n","num_cols_review = ['Rating', 'Pictures', 'No_of_reviews', 'Followers']\n"," #Distribution plot\n","n=1\n","plt.figure(figsize=(15,10))\n","for col in num_cols_review:\n","   plt.subplot(2,2,n)\n","   n+=1\n","   sns.distplot(reviews[col])\n","   plt.title(col)\n","   plt.tight_layout()"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["To see the distribution of numerical columns."],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["Most restaurants have higher ratings, showing generally good customer satisfaction. However, pictures, reviews, and followers are highly right-skewed"],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["the skewness indicates unequal popularity—restaurants with low engagement may struggle to attract customers if they do not improve marketing and customer experience."],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["#Get the count of hour in which customers have given review\n","hr_count=pd.DataFrame(reviews.groupby(by='Hour',as_index=False)['Hour'].count().reset_index(drop=False))\n","hr_count.rename(columns={'index':'Hour','Hour':'Count'},inplace=True)\n","hr_count\n"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualizing through bar plot\n","plt.figure(figsize=(15,10))\n","a=sns.barplot(x='Hour',y='Count',data=hr_count,palette='terrain_r',order=hr_count.sort_values('Count',ascending=False)['Hour'])\n","a.set_xticks(range(len(hr_count)))"],"metadata":{"id":"yW1-UdOBFtbs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["To see the count the number of review for restauants given by each hour."],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["The frequency is higher during the night time from hour 21 to 23, i.e., from 9:00 pm to 11:00 pm and in the afternoon it is peak at 14 hour i.e 2:00 pm. Possibly because people prefer to order food during these hours."],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["This chart helps businesses know the busiest hours, mainly in the evening and night, so they can plan staff, kitchen work, and offers better to earn more.\n","\n","Very slow hours can waste money if restaurants stay fully open without enough customers, which can hurt profit."],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["# No of reviews given to the restaurants\n","res_review=reviews[['Restaurant', 'No_of_reviews']].sort_values(by = 'No_of_reviews', ascending = False).head(10).reset_index(drop=True)\n","res_review"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# No of reviews for each restaurant\n","plt.figure(figsize = (15,15))\n","ax = sns.barplot(x = 'No_of_reviews',y = 'Restaurant',data = res_review ,palette = 'viridis')\n"],"metadata":{"id":"R976WLHAID1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["To check which restaurants are given most number of reviews by the customers."],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["The chart shows that a few restaurants like Labonel, Pista House, and Collage – Hyatt Hyderabad Gachibowli receive the highest number of reviews, indicating high customer engagement and popularity."],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["This helps identify top-performing restaurants,restaurants with very few reviews may face low visibility and slower growth if not addressed."],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14"],"metadata":{"id":"uY4PUV5vKGQ_"}},{"cell_type":"code","source":["# Top 10 average rating restaurants\n","sns.barplot(x='Average_rating', y='Restaurant', data=hotels.sort_values(ascending=False, by='Average_rating')[:10],palette ='coolwarm' )\n","plt.title('10 Most Rated Restaurant')\n","\n","plt.show()"],"metadata":{"id":"1vU4IouaKKGE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"_lV8pRFGKRkd"}},{"cell_type":"markdown","source":["To see the top 10 restaurants having highest average rating."],"metadata":{"id":"OB1CMh6MKeTy"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"sks0evWJKigO"}},{"cell_type":"markdown","source":["AB's - Absolute Barbecues is the top average rated restaurant followed by B-Dubs and 3B's - Buddies, Bar and Barbeque."],"metadata":{"id":"Ip9A2JBGKmim"}},{"cell_type":"markdown","source":["#### Chart - 15 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# checking heatmap/correlation matrix to see the how the colums are correlated with each other\n","f, ax = plt.subplots(figsize=(20,10))\n","sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='crest', linewidths=1)\n","plt.show()"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["To see the correlation among numerical features.Answer Here."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["* Number of reviews and followes has correlation of 0.47 which can be considered as moderate.\n","\n","* Similarly cost and number of cuisines has moderate correlation of 0.4 and cost and average rating have correlation of 0.42\n","\n","* There is low correlation between:\n","\n","  * Pictures and Followers\n","  * Pictures and No of reviews\n","  * Cost and year\n","\n","Other features have very low correlation.Answer Here"],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 16 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","sns.pairplot(df)"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["A pair plot is chosen because it helps visualize the relationships between multiple numerical variables at once. It shows both individual feature distributions and how each pair of features is correlated, making it easy to identify trends, patterns, clusters, and outliers in a single chart."],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["* Most variables are right-skewed, such as followers, number of reviews, and pictures, showing that only a few restaurants get very high engagement.\n","* Cost and rating do not show a very strong linear relationship, indicating higher cost does not always guarantee higher ratings."],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["Null hypothesis H0: There is no relationship between the cost of restaurant and the rating it receives.\n","\n","Alternative hypothesis H1: There is a positive relationship between the cost of a restaurant and the rating it receives.\n","\n","Test : Simple Linear Regression Analysis"],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","import statsmodels.formula.api as smf\n","\n","# fit the linear model\n","model = smf.ols(formula='Rating ~ Cost', data= df).fit()\n","\n","# Check p-value of coefficient\n","p_value = model.pvalues[1]\n","print(\"p_value value is \" + str(p_value))\n","if p_value <= 0.05:\n","  print('Reject null hypothesis')\n","else:\n","  print('Fail to reject null hypothesis')"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["I have used Linear regression test for checking the relationship between the cost of a restaurant and its rating"],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["I chose this test because it is a common and straightforward method for testing the relationship between two continuous variables. This would involve fitting a linear model with the rating as the dependent variable and the cost as the independent variable. The p-value of the coefficient for the cost variable can then be used to determine if there is a statistically significant relationship between the two variables."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Null Hypothesis H0 : There is no relation between number of cuisines and cost.\n","\n","Alternative Hypothesis H1 : Restaurants which serve higher number of cuisines are more costly.\n","\n","Test: chi-square contingency test"],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy.stats import chi2_contingency\n","\n","# defining the table\n","sample = [hotels['No_of_cuisine'], hotels['Cost']]\n","stat, p_value, dof, expected = chi2_contingency(sample)\n","\n","# interpret p_value-value\n","print(\"p_value value is \" + str(p_value))\n","if p_value <= 0.05:\n","    print('Reject null hypothesis')\n","else:\n","    print('Failed to reject null hypothesis')"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["I have used the chi-square contingency test to check if cost and Number of cuisines have relationship or not."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":[" I choose this test because it is suitable for comparing the relationship between two categorical variables. This would involve creating a contingency table with the number of cuisines and the rating of the restaurant have a realtionship or not."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Null hypothesis H0: The variety of cuisines offered by a restaurant has no effect on its rating.\n","\n","Alternative hypothesis H1: The variety of cuisines offered by a restaurant has a positive effect on its rating.\n","\n","Test : Chi-Squared Test"],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","# Perform Statistical Test to obtain P-Value\n","from scipy.stats import chi2_contingency\n","\n","# create a contingency table\n","ct = pd.crosstab(df['Cuisines'], df['Rating'])\n","\n","# perform chi-squared test\n","chi2, p_value, dof, expected = chi2_contingency(ct)\n","\n","# Check p-value\n","print(\"p_value value is \" + str(p_value))\n","if p_value <= 0.05:\n","    print(\"Reject Null Hypothesis\")\n","else:\n","    print(\"Fail to reject Null Hypothesis\")"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["I have used chi-squared test for independence to test the relationship between the variety of cuisines offered by a restaurant and its rating."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":[" I choose this test because it is suitable for comparing the relationship between two categorical variables. This would involve creating a contingency table with the number of restaurants that offer each cuisine as the rows and the rating of the restaurant as the columns."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","# Hotel Dataset\n","null_count = hotels.isnull().sum()\n","null_percent = hotels.isnull().sum() / hotels.shape[0] * 100\n","\n","null_hotel_df = pd.DataFrame({\n","    'Missing_Count': null_count,\n","    'Missing_Percentage (%)': null_percent\n","})\n","\n","print(null_hotel_df)"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking for one missing value in Timings\n","hotels[hotels['Timings'].isnull()]"],"metadata":{"id":"J4nC_ODiQEat"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imputing timings missing value with mode of that column\n","hotels.Timings.fillna(hotels.Timings.mode()[0], inplace = True)"],"metadata":{"id":"RxM_WGKnQOwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dropping collection column since has more than 50% of null values\n","hotels.drop('Collections', axis = 1, inplace = True)"],"metadata":{"id":"lV1rGScHQO5a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["For the hotel dataset or metadataset :\n","* we have drop the collection column becuase it have almost 50% of null values\n","* Their was 1 null value for the timing we have filled that value with the mode because all the hotels will have almost same opening and closing time."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","# review Dataset\n","null_count = reviews.isnull().sum()\n","null_percent = reviews.isnull().sum() / reviews.shape[0] * 100\n","\n","null_review_df = pd.DataFrame({\n","    'Missing_Count': null_count,\n","    'Missing_Percentage (%)': null_percent\n","})\n","\n","print(null_review_df)"],"metadata":{"id":"AJlQVL3rRMH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#filling null values in review_df  review column\n","reviews = reviews.fillna({\"Review\": \"No Review\"})"],"metadata":{"id":"16IY0K4NRMkd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There were missing values in review column, filled it with 'No review'. As this column was having the data of reviews given by the customer to the restaurant."],"metadata":{"id":"pCdkwJB0SB_K"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["#function to plot for outlier detection\n","def outlier_plots(df, features):\n","  for i in range(0,len(features)):\n","    plt.figure(figsize = (20,10))\n","    plt.subplot(1,3,1)\n","    sns.distplot(df[features[i]])\n","    plt.subplot(1,3,2)\n","    plt.scatter(range(df.shape[0]), np.sort(df[features[i]].values))\n","    plt.subplot(1,3,3)\n","    sns.boxplot(df[features[i]])"],"metadata":{"id":"n7emA5GnS3MO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting outliers for review dataset\n","outlier_plots(reviews,['Followers','Pictures','No_of_reviews'])"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# getting outliers for hotel dataset\n","outlier_plots(hotels,['Cost','No_of_cuisine'])"],"metadata":{"id":"8OwpKH5oTvlF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import IsolationForest\n","isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n","isolation_forest.fit(hotels['Cost'].values.reshape(-1, 1))\n","\n","xx = np.linspace(hotels['Cost'].min(), hotels['Cost'].max(), len(hotels)).reshape(-1,1)\n","anomaly_score = isolation_forest.decision_function(xx)\n","outlier = isolation_forest.predict(xx)\n","plt.figure(figsize=(10,4))\n","plt.plot(xx, anomaly_score, label='anomaly score')\n","plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n","where=outlier==-1, color='g',\n","alpha=.4, label='outlier region')\n","plt.legend()\n","plt.ylabel('anomaly score')\n","plt.xlabel('Cost')\n","plt.show();"],"metadata":{"id":"jjI9krSXT-4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n","isolation_forest.fit(reviews['No_of_reviews'].values.reshape(-1, 1))\n","\n","xx = np.linspace(reviews['No_of_reviews'].min(), reviews['No_of_reviews'].max(), len(hotels)).reshape(-1,1)\n","anomaly_score = isolation_forest.decision_function(xx)\n","outlier = isolation_forest.predict(xx)\n","plt.figure(figsize=(10,4))\n","plt.plot(xx, anomaly_score, label='anomaly score')\n","plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n","where=outlier==-1, color='g',\n","alpha=.4, label='outlier region')\n","plt.legend()\n","plt.ylabel('anomaly score')\n","plt.xlabel('No_of_reviews')\n","plt.show();"],"metadata":{"id":"-rw2vv5vT_NT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n","isolation_forest.fit(reviews['Followers'].values.reshape(-1, 1))\n","\n","xx = np.linspace(reviews['Followers'].min(), reviews['Followers'].max(), len(hotels)).reshape(-1,1)\n","anomaly_score = isolation_forest.decision_function(xx)\n","outlier = isolation_forest.predict(xx)\n","plt.figure(figsize=(10,4))\n","plt.plot(xx, anomaly_score, label='anomaly score')\n","plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n","where=outlier==-1, color='g',\n","alpha=.4, label='outlier region')\n","plt.legend()\n","plt.ylabel('anomaly score')\n","plt.xlabel('Followers')\n","plt.show();"],"metadata":{"id":"ip_4bUVrVadb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For Skew Symmetric features defining upper and lower boundry\n","\n","def outlier_treatment_skew(df,feature):\n","  #inter quartile range\n","  IQR= df[feature].quantile(0.75) - df[feature].quantile(0.25)\n","  lower_bound = df[feature].quantile(0.25) - 1.5*IQR\n","  upper_bound = df[feature].quantile(0.75) + 1.5*IQR\n","  return upper_bound,lower_bound"],"metadata":{"id":"mozuUeOHV43F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def replace_outliers(df,features):\n","\n","  #lower limit capping\n","  df.loc[df[features]<= outlier_treatment_skew(df=df,feature=features)[1],features] = outlier_treatment_skew(df=df,feature=features)[1]\n","\n","  #upper limit capping\n","  df.loc[df[features]>= outlier_treatment_skew(df=df,feature=features)[0],features] = outlier_treatment_skew(df=df,feature=features)[0]"],"metadata":{"id":"EAgxlbPDV5FC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace the outlier value with its upper bound and lower bound\n","replace_outliers(hotels,'Cost')\n","replace_outliers(reviews,'No_of_reviews')\n","replace_outliers(reviews,'Followers')"],"metadata":{"id":"y7u0NOr8V5Q7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["Since cost,reviewer and follower feature or column show positive skewed distribution and using isolation forest found they have outliers, hence using the capping technique, instead of removing the outliers and capped outliers with the highest and lowest limit using IQR method by replacing the upper with upper limit and lower with the lowe limit."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["# Encode your categorical columns\n","# create the new dataframe for clustering\n","# And have encoding on cuisines\n","cluster_df = hotels.drop([ 'Timings'],axis=1)\n","# Encode your categorical columns\n","cluster_df['Cuisines'] = cluster_df['Cuisines'].str.split(',')\n","\n","#using explode converting list to unique individual items\n","cluster_df = cluster_df.explode('Cuisines')\n","\n","#removing extra trailing space from Cuisines after exploded\n","cluster_df['Cuisines'] = cluster_df['Cuisines'].apply(lambda x: x.strip())\n","\n","#using get dummies to get dummies for Cuisines\n","cluster_df = pd.get_dummies(cluster_df, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n","\n","cluster_df = cluster_df.groupby(\"Restaurant\").sum().reset_index()"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["For encoding of categorical feature which is 'Cuisines' , First I have splitted the cuisines into a list and then created dummy variables for each of the cuisines."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction\n","# creating datafame for sentiment analysis\n","sentiment_df = reviews[['Review', 'Rating']]"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"A5joVCqvZh71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import contractions\n","# applying fuction for contracting text\n","sentiment_df['Review']=sentiment_df['Review'].apply(lambda x:contractions.fix(str(x)))"],"metadata":{"id":"SoSAe-h9ZiLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","sentiment_df['Review'] = sentiment_df['Review'].str.lower()\n","sentiment_df.head()"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","import string\n","def remove_punctuation(text):\n","  '''This function is for removing punctuation'''\n","   # replacing the punctuations with no space, hence punctuation marks will be removed\n","  translator = text.translate(str.maketrans('', '', string.punctuation))\n","  # return the text stripped of punctuation marks\n","  return (translator)"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove punctuation using function created\n","sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)\n","sentiment_df.head()"],"metadata":{"id":"sEmKkh5daPAd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","#function to remove digits\n","def remove_digit(text):\n","  '''Function to remove digit from text'''\n","  char_str = '' .join((z for z in text if not z.isdigit()))\n","  return char_str"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#remove digit using function created\n","sentiment_df['Review'] = sentiment_df['Review'].apply(remove_digit)\n","sentiment_df.head()"],"metadata":{"id":"W20yo6YNajTz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["# Remove Stopwords\n","# Remove Stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","sw = stopwords.words('english')"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_stopwords(text):\n","    '''a function for removing the stopword'''\n","    # removing the stop words and lowercasing the selected words\n","    text = [word.lower() for word in text.split() if word.lower() not in sw]\n","    # joining the list of words with space separator\n","    return \" \".join(text)\n","\n","sentiment_df['Review'] = sentiment_df['Review'].apply(remove_stopwords)"],"metadata":{"id":"eaMryYtAa-Xt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_df.head()"],"metadata":{"id":"c3exkM6la-lq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))\n","sentiment_df.head()"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["No required in my analysis"],"metadata":{"id":"PFJ9JZvxbsp3"}},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Tokenization\n","import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)\n","sentiment_df.sample(10)"],"metadata":{"id":"n578ovGOcawi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n","#applying Lemmatization\n","from nltk.stem import WordNetLemmatizer\n","\n","# Create a lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","def lemmatize_tokens(tokens):\n","  '''function for lemmatization'''\n","  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","  return lemmatized_tokens\n","\n","# Lemmatize the 'Review' column\n","sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["I have used Lemmatization text normalization technique.\n","\n","Lemmatization converts words into their meaningful base or dictionary form while preserving the context of the word. Unlike stemming, which may cut words incorrectly, lemmatization produces linguistically correct words. This helps reduce vocabulary size, improves model understanding, and leads to better performance in sentiment analysis by maintaining semantic meaning.\n"],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging\n","#Lemmatization without POS tagging is sufficient for reducing words to their base form."],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Vectorizing Text\n","from sklearn.feature_extraction.text import  TfidfVectorizer\n","vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n","X = sentiment_df['Review']\n","X= vectorizer.fit_transform(X)"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["Here I have used Tf-idf Vectorization technique.\n","\n","TF-IDF (term frequency-inverse document frequency) is a technique that assigns a weight to each word in a document. It is calculated as the product of the term frequency (tf) and the inverse document frequency (idf).\n","\n","The term frequency (tf) is the number of times a word appears in a document, while the inverse document frequency (idf) is a measure of how rare a word is across all documents in a collection. The intuition behind tf-idf is that words that appear frequently in a document but not in many documents across the collection are more informative and thus should be given more weight.\n","\n","The mathematical formula for tf-idf is as follows:\n","\n","tf-idf(t, d, D) = tf(t, d) * idf(t, D)\n","\n","where t is a term (word), d is a document, D is a collection of documents, tf(t, d) is the term frequency of t in d, and idf(t, D) is the inverse document frequency of t in D."],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","hotels.columns"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hotels.drop('Links',axis=1,inplace=True)"],"metadata":{"id":"oiowVY6Ke531"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews.columns"],"metadata":{"id":"ciKJ0fo6fKM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for sentiment analysis, creating dependant variable based on rating\n","#We will create 2 categories based on the rating by creating a python function\n","def sentiment(rating):\n","  if rating >=3.5:\n","    return 1\n","    # positive sentiment\n","  else:\n","    return 0\n","    # negative sentiment"],"metadata":{"id":"5R5HmnILfKFf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# applying to sentiment dataset\n","sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(sentiment)\n","sentiment_df.head()"],"metadata":{"id":"1Oc__6q5fJ7c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","print('for sentiment analysis : ',sentiment_df.columns)\n","# For clustering analysis\n","print('\\nFor clustering analysis :', cluster_df.columns)"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["The features will be selected usign PCA feature selection,beneficial while using Dimensionality reduction technique."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["# Transform Your data\n","#check if data needs to be transformed\n","def skewed_feature(df,features):\n","  symmetric_f = []\n","  skewed_f = []\n","  for i in features:\n","      if (df[i].skew() <= -1) | (df[i].skew() >= 1) :\n","        skewed_f.append(i)\n","      else:\n","        symmetric_f.append(i)\n","  return symmetric_f, skewed_f"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#finding symmetric and skew symmetric features IN CLUSTER DF\n","features = ['Cost', 'No_of_cuisine', 'Average_rating']\n","s,sk=skewed_feature(cluster_df,features)\n","print('Symmetric features :',s)\n","print('Skew symmetric features :',sk)"],"metadata":{"id":"ABIUU-HRg83P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#finding symmetric and skew symmetric features in Sentiment DF\n","features=['Rating', 'Sentiment']\n","s,sk=skewed_feature(sentiment_df,features)\n","print('Symmetric features :',s)\n","print('Skew symmetric features :',sk)"],"metadata":{"id":"H5heTWj6g8ux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform Your data\n","#Cost is skewed symmetric. Hence we have applied log transformation on Cost.\n","cluster_df['Cost'] = np.log1p(cluster_df['Cost'])"],"metadata":{"id":"B7AaOsY2iex6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization of log transformation of cost\n","sns.distplot(cluster_df['Cost'], color = '#055E85')\n","plt.axvline(cluster_df['Cost'].mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n","plt.axvline(cluster_df['Cost'].median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n","plt.legend(bbox_to_anchor = (1.0, 1), loc = 'best')\n","plt.title('Cost');\n","plt.tight_layout();"],"metadata":{"id":"koZwONRLhIEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["cluster_df.head()"],"metadata":{"id":"0rqlsgSzjYKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scaling your data\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","#Min max scaler for only numeric columns"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaled_df = cluster_df.copy()\n","scaled_df[[\"Cost\",\"No_of_cuisine\",\"Average_rating\"]] = scaler.fit_transform(cluster_df[[\"Cost\",\"No_of_cuisine\",\"Average_rating\"]])\n","scaled_df.set_index(\"Restaurant\", inplace= True)"],"metadata":{"id":"7_GnloqZi4om"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applying minmax transformation to numeric data\n","numeric_cols = list(cluster_df.describe().columns)\n","scaled_df = pd.DataFrame(scaler.fit_transform(cluster_df[numeric_cols]))\n","scaled_df.columns = numeric_cols"],"metadata":{"id":"TKObwyV2i4_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaled_df.head()"],"metadata":{"id":"2V1Zu9m5i5NG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["I have used MinMax Scaler to scale the data. The feature scaling is used to prevent the models from getting biased toward a specific range of values. Since the dummy variables created from cuisines contains the value 0 and 1 while other variables have different range of values."],"metadata":{"id":"dc72CRFsmnIB"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Yes, it is important to use dimensionality reduction techniques as dataset has 40 or more features. This is because, as the number of features increases, the computational cost of clustering algorithms also increases. Dimensionality reduction techniques such as PCA can help reduce the number of features while maintaining the important information in the data, making it easier to cluster and interpret the results."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)\n","#applying pca\n","\n","features = scaled_df.columns\n","\n","# create an instance of PCA\n","from sklearn.decomposition import PCA\n","pca = PCA()\n","\n","# fit PCA on features\n","pca.fit(scaled_df)"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#explained variance v/s no. of components\n","plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n","plt.xlabel('Number of components',size = 15, color = 'red')\n","plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n","plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n","plt.xlim([0, 20])\n","plt.show()"],"metadata":{"id":"AipEKO5YnUH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#using n_component as 4\n","pca = PCA(n_components=3)\n","\n","# fit PCA on features\n","pca.fit(scaled_df)\n","\n","# explained variance ratio of each principal component\n","print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n","# variance explained by three components\n","print('Cumulative variance explained by 4 principal components: {:.2%}'.format(\n","                                        np.sum(pca.explained_variance_ratio_)))\n","\n","# transform data to principal component space\n","df_pca = pca.transform(scaled_df)"],"metadata":{"id":"ejpM4xornUX1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["I have used PCA as dimension reduction technique, because PCA (Principal Component Analysis) is a widely used dimensionality reduction technique because it is able to identify patterns in the data that are responsible for the most variation.Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features.\n","\n"],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","# for sentiment analysis using sentiment_df dataframe\n","#X = X_tfidf\n","y = sentiment_df['Sentiment']"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.value_counts()"],"metadata":{"id":"ZPXu1KEES0AV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#spliting test train\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30)\n","\n","# describes info about train and test set\n","print(\"X_train \", X_train.shape)\n","print(\"y_train \", y_train.shape)\n","print(\"X_test \", X_test.shape)\n","print(\"y_test \", y_test.shape)"],"metadata":{"id":"p4i-WFhtS0FE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = X_train.toarray()\n","X_test = X_test.toarray()"],"metadata":{"id":"uWebHHjjS8bW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["I have used 70:30 split which is one the most used split ratio. Since there was only 9961 data, therefore I have used more in training set."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Yes the dataset is imbalanced but since it is slightly imbalanced, hence handling is not neccesaary. So we can proceed with same dataset."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# check if dataset is imbalanced or not\n","sentiment_df['Sentiment'].value_counts().plot(kind='pie',\n","                               autopct=\"%1.1f%%\",\n","                               labels=['Positive Sentiment','Negative Sentiment'],\n","                               colors=['green','red'],\n","                               explode=[0.01,0.02])"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"markdown","source":["**K Means clustering**\n","\n","K-Means Clustering is an Unsupervised Learning algorithm.The algorithm takes the unlabeled dataset as input, divides the dataset into k-number of clusters, and repeats the process until it does not find the best clusters."],"metadata":{"id":"9lgQMhjgTalW"}},{"cell_type":"code","source":["df_pca_copy = df_pca.copy()"],"metadata":{"id":"hH0Og9wYToe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","#importing kmeans\n","from sklearn.cluster import KMeans\n","# Fit the Algorithm\n","wcss_list= []  #Initializing the list for the values of WCSS\n","wcss_dict = {}\n","#Using for loop for iterations from 1 to 10.\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)\n","    kmeans.fit(df_pca)\n","    wcss_list.append(kmeans.inertia_)\n","    wcss_dict[i] = kmeans.inertia_\n","\n","wcss_dict"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot for sum of squared distance for each number of cluster\n","plt.plot(range(1, 11), wcss_list)\n","plt.plot(range(1,11),wcss_list, linewidth=2, color=\"green\", marker =\"o\")\n","plt.title('The Elobw Method Graph')\n","plt.xlabel('Number of clusters(k)')\n","plt.ylabel('wcss_list')\n","plt.show()"],"metadata":{"id":"nJoER9Z9TygW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# silhoutte score to find optimal number of scores\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import silhouette_samples\n","from sklearn.model_selection import ParameterGrid\n","\n","silhouette_avg =[]\n"," # Calculate average silhouette score for each number of clusters (2 to 10)\n","\n","for k in range(2,11):\n","  km = KMeans(n_clusters=k, random_state=3)\n","  km.fit(df_pca)\n","  silhouette_avg.append(silhouette_score(df_pca, km.labels_))\n","\n","# plot the results\n","plt.plot(range(2,11), silhouette_avg)\n","plt.xlabel('Number of clusters')\n","plt.ylabel('Silhouette Coefficient')\n","plt.grid(True)"],"metadata":{"id":"Lr8MssRRTyjv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.colors import ListedColormap\n","import matplotlib.cm as cm\n","#visualizing Silhouette Score for individual clusters and the clusters made\n","range_n_clusters = [2, 3, 4, 5, 6]\n","\n","for n_clusters in range_n_clusters:\n","    # Create a subplot with 1 row and 2 columns\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    fig.set_size_inches(18, 7)\n","\n","    # The 1st subplot is the silhouette plot\n","    # The silhouette coefficient can range from -1, 1\n","    ax1.set_xlim([-1, 1])\n","    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n","    # plots of individual clusters, to demarcate them clearly.\n","    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n","\n","    # Initialize the clusterer with n_clusters value and a random generator\n","    # seed of 10 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n","    cluster_labels = clusterer.fit_predict(df_pca)\n","\n","    # The silhouette_score gives the average value for all the samples.\n","    # This gives a perspective into the density and separation of the formed\n","    # clusters\n","    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n","    print(\"For n_clusters =\", n_clusters,\n","          \"The average silhouette_score is :\", silhouette_avg)\n","\n","    # Compute the silhouette scores for each sample\n","    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n","\n","    y_lower = 10\n","    for i in range(n_clusters):\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = \\\n","            sample_silhouette_values[cluster_labels == i]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                          0, ith_cluster_silhouette_values,\n","                          facecolor=color, edgecolor=color, alpha=0.7)\n","\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","        # Compute the new y_lower for next plot\n","        y_lower =  y_upper + 10  # 10 for the 0 samples\n","\n","    ax1.set_title(\"The silhouette plot for the various clusters.\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","    # 2nd Plot showing the actual clusters formed\n","    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n","                c=colors, edgecolor='k')\n","\n","    # Labeling the clusters\n","    centers = clusterer.cluster_centers_\n","    # Draw white circles at cluster centers\n","    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n","                c=\"white\", alpha=1, s=200, edgecolor='k')\n"],"metadata":{"id":"iqkhru2HTync"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fitting on 6 clusters\n","kmeans = KMeans(n_clusters=6, init='k-means++', random_state= 10)\n","y_predict= kmeans.fit_predict(df_pca)"],"metadata":{"id":"EWiiSJmLTyra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visulaizing the clusters\n","plt.figure(figsize=(15,10))\n","plt.scatter(df_pca[y_predict == 0, 0], df_pca[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') #for first cluster\n","plt.scatter(df_pca[y_predict == 1, 0], df_pca[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') #for second cluster\n","plt.scatter(df_pca[y_predict== 2, 0], df_pca[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') #for third cluster\n","plt.scatter(df_pca[y_predict == 3, 0], df_pca[y_predict == 3, 1], s = 100, c = 'orange', label = 'Cluster 4') #for fourth cluster\n","plt.scatter(df_pca[y_predict == 4, 0], df_pca[y_predict == 4, 1], s = 100, c = 'purple', label = 'Cluster 5') #for first cluster\n","plt.scatter(df_pca[y_predict == 5, 0], df_pca[y_predict == 5, 1], s = 100, c = 'magenta', label = 'Cluster 6') #for second cluster\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 150, c = 'black', label = 'Centroid')\n","plt.title('Clusters of Restaurants')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"moJx6H-6Tyvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assigning clusters to our data\n","new_df_cluster = cluster_df.copy()\n","cluster_df['clusters'] = y_predict\n","# checking how it is working\n","cluster_df.head()"],"metadata":{"id":"s4oI4J5QTyzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# count of each of 6 clusters\n","cluster_df['clusters'].value_counts()"],"metadata":{"id":"vTPK5LYYTy3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating new df for checkign cuisine in each cluster\n","new_cluster_df = hotels.copy()\n","new_cluster_df['clusters'] = y_predict\n","new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].str.split(',')\n","new_cluster_df = new_cluster_df.explode('Cuisines')\n","\n","#removing extra trailing space from cuisines after exploded\n","new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].apply(lambda x: x.strip())\n","new_cluster_df.head(10)"],"metadata":{"id":"d60VY2mVVgbh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_cluster_df.shape"],"metadata":{"id":"sRXA--coVwP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#printing cuisine list for each cluster\n","for cluster in new_cluster_df['clusters'].unique().tolist():\n","  print('Cuisine List for Cluster :', cluster,'\\n')\n","  print(new_cluster_df[new_cluster_df[\"clusters\"]== cluster]['Cuisines'].unique(),'\\n')\n","  print('\\n')"],"metadata":{"id":"Tn9diCVZVwUf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"markdown","source":["ELBOW METHOD\n","This method uses the concept of WCSS value. WCSS stands for Within Cluster Sum of Squares, which defines the total variations within a cluster.\n","\n","SILHOUETTE METHOD\n","The silhouette coefficient or silhouette score kmeans is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation)."],"metadata":{"id":"rrHuAN8rWaJQ"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"markdown","source":["**Hierarchical clustering**"],"metadata":{"id":"-Ln-7XIqWpnE"}},{"cell_type":"code","source":["#importing module for hierarchial clustering and vizualizing dendograms\n","import scipy.cluster.hierarchy as sch\n","plt.figure(figsize=(10,5))\n","dendrogram = sch.dendrogram(sch.linkage(df_pca, method = 'ward'),orientation='top',\n","            distance_sort='descending',\n","            show_leaf_counts=True)\n","\n","plt.title('Dendrogram')\n","plt.xlabel('Restaurants')\n","plt.ylabel('Euclidean Distances')\n","\n","plt.show()"],"metadata":{"id":"JxMLGKlzWzDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Checking the Silhouette score for 8 clusters\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.metrics import silhouette_score\n","\n","range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n","\n","for n_clusters in range_n_clusters:\n","    hc = AgglomerativeClustering(\n","        n_clusters=n_clusters,\n","        linkage='ward'\n","    )\n","    y_hc = hc.fit_predict(df_pca)\n","    score = silhouette_score(df_pca, y_hc)\n","    print(f\"For n_clusters = {n_clusters}, silhouette score is {score}\")\n"],"metadata":{"id":"tBLzTWfmWzHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import AgglomerativeClustering\n","\n","# define the model\n","model = AgglomerativeClustering(n_clusters=6,\n","        linkage='ward')\n","\n","#fit and predict on model\n","y_predict = model.fit_predict(df_pca)"],"metadata":{"id":"4Y0KrvgbWzL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the clusters\n","plt.figure(figsize=(15,10))\n","plt.scatter(df_pca[y_predict == 0,0], df_pca[y_predict == 0,1], s=100, c='cyan')\n","plt.scatter(df_pca[y_predict == 1,0], df_pca[y_predict == 1,1], s=100, c='red')\n","plt.scatter(df_pca[y_predict == 2,0], df_pca[y_predict == 2,1], s=100, c='blue')\n","plt.scatter(df_pca[y_predict == 3,0], df_pca[y_predict == 3,1], s=100, c='green')\n","plt.scatter(df_pca[y_predict == 4,0], df_pca[y_predict == 4,1], s=100, c='orange')\n","plt.scatter(df_pca[y_predict == 5,0], df_pca[y_predict == 5,1], s=100, c='magenta')\n"],"metadata":{"id":"SXVm4LRSWzQs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"markdown","source":["In this project, Agglomerative Hierarchical Clustering was used as the primary machine learning model. It is an unsupervised learning algorithm that groups data points based on their similarity without using labeled data. The model starts by treating each data point as an individual cluster and then progressively merges the closest clusters until the desired number of clusters is formed."],"metadata":{"id":"-3Io1-O7YScI"}},{"cell_type":"markdown","source":["#### 2. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Hierarchical Clustering helps businesses group data based on similarities and differences, making it easier to understand customer segments and their unique characteristics. This allows companies to tailor pricing, products, services, and marketing strategies for each segment. Well-defined segments enable more personalized targeting, leading to improved customer engagement and better business performance in the market.\n"],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"markdown","source":["**Sentimental Analysis**"],"metadata":{"id":"qHHYbf4UZWkG"}},{"cell_type":"code","source":["#Importing all the required libraries for sentiment analysis\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,train_test_split\n","from sklearn.metrics import accuracy_score,confusion_matrix,f1_score\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, roc_auc_score, roc_curve"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of models\n","models = [[\"LogisticRegression\", LogisticRegression(fit_intercept = True, class_weight='balanced')], [\"DecisionTree\", DecisionTreeClassifier()],\n","          [\"RandomForest\",RandomForestClassifier()],[\"XGBoost\", XGBClassifier()],\n","          [\"KNN\", KNeighborsClassifier()]]"],"metadata":{"id":"iLvLWo4EZkx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#function for fitting the model and calculating scores\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import pandas as pd\n","\n","def model_build(models, X_train, X_test, y_train, y_test):\n","\n","    score_matrix = pd.DataFrame()\n","    roc_sc = {}\n","\n","    for model_name, model in models:\n","        current_result = {}\n","\n","        # Fit model\n","        model.fit(X_train, y_train)\n","\n","        # Predictions\n","        y_pred_test = model.predict(X_test)\n","        y_pred_train = model.predict(X_train)\n","        ypredProb = model.predict_proba(X_test)\n","\n","        # Metrics\n","        current_result[\"Model\"] = model_name\n","        current_result[\"Train Accuracy\"] = accuracy_score(y_train, y_pred_train)\n","        current_result[\"Test Accuracy\"] = accuracy_score(y_test, y_pred_test)\n","        current_result[\"Test Precision\"] = precision_score(y_test, y_pred_test)\n","        current_result[\"Test Recall\"] = recall_score(y_test, y_pred_test)\n","        current_result[\"Test F1\"] = f1_score(y_test, y_pred_test)\n","        current_result[\"Test ROC_AUC Score\"] = roc_auc_score(y_test, ypredProb[:,1])\n","\n","        # Convert to DataFrame\n","        current_result = pd.DataFrame([current_result])\n","\n","        # Correct concat\n","        score_matrix = pd.concat([score_matrix, current_result], ignore_index=True)\n","\n","        # ROC Curve values\n","        fpr, tpr, threshold = roc_curve(y_test, ypredProb[:,1])\n","        roc_sc[model_name] = (fpr, tpr)\n","\n","    # Random ROC baseline\n","    random_probs = [0 for _ in range(len(y_test))]\n","    p_fpr, p_tpr, _ = roc_curve(y_test, random_probs)\n","    roc_sc[\"TPR = FPR\"] = (p_fpr, p_tpr)\n","\n","    return score_matrix, roc_sc"],"metadata":{"id":"R4fUC2N_Zk4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtaining results\n","model_results, Curve = model_build(models,X_train,X_test,y_train,y_test)"],"metadata":{"id":"D2qcg4jOZlBm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_results"],"metadata":{"id":"fNnvj6tKZlGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ROC_AUC curve\n","plt.figure(figsize = (10,7))\n","for model , value in Curve.items():\n","  sns.lineplot(value[1], label = model)"],"metadata":{"id":"ejhQiQiFZlKy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Based on the ROC curve visualization, Logistic Regression is the best-performing model and selected for final deployment."],"metadata":{"id":"tXzAXkQ-gV0B"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# hyperparameter tuning\n","from sklearn.model_selection import GridSearchCV"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_grid = [\n","    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n","    'C' : [100, 10, 1.0, 0.1, 0.01],\n","    'solver' : ['lbfgs','newton-cg','liblinear'],\n","    }\n","]\n","\n"],"metadata":{"id":"uPAB4eAKhZAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid_lr = GridSearchCV( LogisticRegression(fit_intercept = True, class_weight='balanced'), param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\n","best_clf = grid_lr.fit(X_train,y_train)\n","\n","# Get the results\n","print(grid_lr.best_score_)\n","print(grid_lr.best_estimator_)\n","print(grid_lr.best_params_)"],"metadata":{"id":"vwA_VzkShZLJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_model = LogisticRegression(random_state=42, solver='lbfgs', penalty= 'l2', C = 10 )\n","final_model.fit(X_train, y_train)"],"metadata":{"id":"lXnrXS1khZao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prediction report\n","y_pred = final_model.predict(X_test)\n","print(classification_report(y_test,y_pred,digits=4))"],"metadata":{"id":"QO-on2nlhZmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion Matrix\n","conf_mat = confusion_matrix(y_test, y_pred)\n","plt.rcParams['figure.figsize'] = (5, 5)\n","sns.heatmap(conf_mat, annot = True, linewidths=.5, cmap=\"YlGnBu\")\n","plt.title('Confusion matrix')\n","plt.show()"],"metadata":{"id":"lDI30PEyhZtE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["I have used Grid Search CV Hyperparameter optimization technique and tried to find the best values of C.I got best params 'C': 10. I have also used Cross validation with CV = 3."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["After the hyperparameter tuning of Logistic Regression we observed the following improvements in the evaluation metrics.\n","\n","Accuracy Before: 85.48% || Accuracy After: 86.00%\n","\n","Precision Before: 90.97% || Precision After:85.97 %\n","\n","Recall Before: 85.96% || Recall After: 86.02%\n","\n","F1 Score Before: 88.40%|| F1 Score After: 85.99%"],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["For sentiment analysis, evaluation metrics used were precision, recall, F1-score, and accuracy.\n","\n","* Precision measures the proportion of true positive predictions among all positive predictions. It is a good metric to use when the cost of false positives is high.\n","\n","* Recall (also known as sensitivity or true positive rate) measures the proportion of true positive predictions among all actual positive instances. It is a good metric to use when the cost of false negatives is high.\n","\n","* F1-score is the harmonic mean of precision and recall, and is a good overall measure of a classifier's performance.\n","\n","* Accuracy is the proportion of correctly classified instances among all instances.\n","\n","The specific evaluation metric to use will depend on the specific use case and the relative costs of false positives and false negatives. For a positive business impact, F1-score can be considered as it balances the precision and recall to give an overall performance measure."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["I have chosen logistic regression model for my final prediction because auc_roc score for logistic regression is highest among other models."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In this project, Zomato restaurant data was analyzed to understand restaurant performance and customer sentiment using data science techniques. Exploratory analysis showed that most restaurants fall in the mid-price range, popular cuisines like North Indian and Chinese dominate the market, and only a few restaurants receive most of the customer engagement.\n","\n","Hierarchical clustering helped group restaurants into meaningful segments, making it easier to understand similarities between them. Silhouette analysis confirmed that six clusters provided the best separation. Sentiment analysis was performed using multiple machine learning models, and Logistic Regression was selected as the final model because it showed the best overall performance and highest ROC-AUC score.\n","\n","Overall, this project demonstrates how restaurant data and customer reviews can be used to gain valuable business insights. These insights can help restaurants improve service quality, pricing strategies, and customer satisfaction, while platforms like Zomato can enhance recommendations and user experience."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}